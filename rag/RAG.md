# Environmental Events RAG System

Real-time environmental intelligence for Bucharest. Scrapes news and stores events in a location-tagged database for your chatbot/agent to query.

## Features

ðŸ” **Multi-Source Ingestion**
- Google News Search (via Serper API) - 6 environmental queries
- RSS Feeds: Romania Insider, HotNews, Digi24, Observator News
- Automatic relevance filtering by keywords

ðŸ“ **Location-Based Storage**
- Fast regex extraction for sectors (1-6) and neighborhoods
- Optional LLM extraction for complex locations
- Spatial database queries (find events within X km)

ðŸ·ï¸ **Event Classification**
- Fire & emergencies
- Traffic congestion & road closures
- Air pollution spikes
- Construction & roadwork
- Festivals & public events
- Weather impacts
- Health alerts

ðŸ§  **Database-First Design**
- Stores events in Supabase PostgreSQL
- Vector embeddings (OpenAI ada-002) for semantic search
- Your agent uses text-to-SQL to query events
- No built-in chatbot - just data ingestion

## Database Schema

Run `database/events_schema.sql` in Supabase SQL Editor to create:

- `environmental_events` table with location, embeddings, event types
- Spatial search functions (`get_events_near_location`)
- Sector-based queries (`get_events_by_sector`)
- Semantic search (`search_events_by_embedding`)
- Auto-cleanup for old events (90 days retention)

## Setup

### 1. Install Dependencies

```bash
cd rag
pip install -r requirements.txt
```

### 2. Configure Environment Variables

Add to your `.env` file:

```env
# OpenAI API (required for embeddings & LLM)
OPENAI_API_KEY=sk-...

# Supabase Database
host=your-project.supabase.co
port=5432
dbname=postgres
user=postgres
password=your-password

# Serper API for Google News Search (optional but recommended)
SERPER_API_KEY=your-serper-key  # Get from https://serper.dev

# NewsAPI (optional, alternative source)
NEWS_API_KEY=your-newsapi-key   # Get from https://newsapi.org
```

### 3. Create Database Tables

```sql
-- Run in Supabase SQL Editor
-- (copy contents from database/events_schema.sql)
```

## Usage

### Ingest Environmental Data

Collect and process events from the last 7 days:

```bash
python rag_ingestion.py ingest 7
```

Collect from last 2 days (faster):

```bash
python rag_ingestion.py ingest 2
```

This will:
1. Search Google News for 6 Bucharest environmental queries
2. Scrape 4 RSS feeds (Romania Insider, HotNews, Digi24, Observator)
3. Filter by event keywords (fire, traffic, pollution, etc.)
4. Extract location using fast regex (sectors, neighborhoods)
5. Create OpenAI embeddings for semantic search
6. Save to `environmental_events` table in Supabase

**Schedule with cron:**
```bash
# Run every 6 hours
0 */6 * * * cd /path/to/rag && python rag_ingestion.py ingest 1
```

### Query Events (From Your Agent)

Your chatbot/agent should query the database directly using text-to-SQL:

```sql
-- Get recent events in Sector 3
SELECT * FROM get_events_by_sector(3, 48);

-- Find events near a location (within 5km)
SELECT * FROM get_events_near_location(44.4268, 26.1025, 5.0, 168);

-- Semantic search by embedding similarity
SELECT * FROM search_events_by_embedding(
    '[your_query_embedding_vector]'::vector,
    168,
    10
);

-- Get all recent high-priority events
SELECT * FROM recent_high_priority_events;
```

**Example: Agent queries database**
```python
import psycopg2

def get_environmental_context(sector: int, hours_ago: int = 48):
    """Get recent environmental events for a sector"""
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT title, content, event_type, severity, published_at, url
        FROM environmental_events
        WHERE sector = %s
        AND published_at >= NOW() - (%s || ' hours')::INTERVAL
        ORDER BY published_at DESC
        LIMIT 10
    """, (sector, hours_ago))
    
    events = cursor.fetchall()
    cursor.close()
    conn.close()
    
    return events
```

## Data Sources

### Active Sources

1. **Google News** (via Serper API - requires key)
   - 6 search queries: poluare, incendiu, trafic blocat, drum Ã®nchis, accident, calitatea aerului
   - Location: Bucharest, Romania
   - Filters by date range

2. **RSS Feeds** (Free, no API key needed)
   - Romania Insider: `https://www.romania-insider.com/feed`
   - HotNews: `https://rss.hotnews.ro/`
   - Digi24: `https://www.digi24.ro/rss`
   - Observator News: `https://observatornews.ro/feed`

### To Add (Future)

3. **Social Media**
   - Instagram API (requires Meta Business account)
   - TikTok API (requires TikTok Developer account)
   - Twitter/X API (requires paid tier)

4. **Official Sources**
   - Bucharest City Hall RSS/API
   - CNAIR road closures feed
   - ISU fire department alerts
   - ANPM pollution warnings

## Event Classification

The system automatically categorizes events:

| Type | Keywords (Romanian & English) |
|------|-------------------------------|
| **fire** | incendiu, foc, flÄƒcÄƒri, smoke, wildfire |
| **traffic** | trafic, ambuteiaj, blocaj, congestion |
| **pollution** | poluare, calitatea aerului, pm2.5, smog |
| **construction** | È™antier, lucrÄƒri, roadwork |
| **festival** | festival, eveniment, concert, parade |
| **road_closure** | drum Ã®nchis, blocked, restricÈ›ii |
| **weather** | vreme, furtunÄƒ, storm, rain |
| **health** | sÄƒnÄƒtate, astm, respiratory, hospital |

## Severity Levels

- **critical**: Deaths, evacuations, toxic hazards
- **high**: Major fires, serious pollution, health alerts
- **medium**: Moderate impact, temporary issues
- **low**: Minor events, informational

## Location Detection

The system extracts:
- **Sector** (1-6): Primary administrative division
- **Neighborhood**: Specific area (e.g., CrangaÈ™i, Titan)
- **GPS Coordinates**: For spatial queries
- **Address**: Street-level if mentioned

## Database Queries

### Find events near a location

```sql
SELECT * FROM get_events_near_location(
    44.4268,  -- latitude
    26.1025,  -- longitude
    5.0,      -- radius in km
    168       -- hours ago (7 days)
);
```

### Get events by sector

```sql
SELECT * FROM get_events_by_sector(
    3,    -- sector number
    48    -- hours ago (2 days)
);
```

### Semantic search

```python
from rag_query import semantic_search_events

events = semantic_search_events(
    "traffic accidents near Piata Unirii",
    hours_ago=24
)
```

## Auto-Cleanup

Events are automatically deleted after:
- **90 days** from publication (configurable)
- **Expiration date** reached (set per event)

Run manual cleanup:

```sql
SELECT cleanup_old_events();
```

## Agent Integration

Your chatbot agent should query the `environmental_events` table directly using text-to-SQL strategy.

### Example: Backend Agent Query

```python
import psycopg2
from openai import OpenAI

client = OpenAI()

def get_environmental_context_for_agent(user_query: str):
    """
    Agent uses text-to-SQL to query environmental events
    """
    # Step 1: Convert user query to SQL
    sql_prompt = f"""
    Generate a SQL query for PostgreSQL database with table 'environmental_events':
    Columns: title, content, event_type[], severity, sector, neighborhood, 
             latitude, longitude, published_at, url, source_name
    
    User question: "{user_query}"
    
    Return ONLY the SQL query, no explanation.
    """
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": sql_prompt}]
    )
    
    sql_query = response.choices[0].message.content.strip()
    
    # Step 2: Execute SQL query
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    cursor.execute(sql_query)
    results = cursor.fetchall()
    cursor.close()
    conn.close()
    
    # Step 3: Format results for agent context
    context = "\n".join([
        f"- {row[0]} ({row[4]}, {row[5]}): {row[1][:200]}"
        for row in results
    ])
    
    return context
```

### Example: React Native App Backend

```python
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/api/environmental-events', methods=['POST'])
def get_events():
    data = request.json
    sector = data.get('sector')
    hours_ago = data.get('hours_ago', 48)
    
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT title, content, event_type, severity, 
               published_at, url, neighborhood
        FROM environmental_events
        WHERE sector = %s
        AND published_at >= NOW() - (%s || ' hours')::INTERVAL
        ORDER BY severity DESC, published_at DESC
        LIMIT 20
    """, (sector, hours_ago))
    
    columns = [desc[0] for desc in cursor.description]
    events = [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    cursor.close()
    conn.close()
    
    return jsonify({'events': events, 'count': len(events)})
```

## Cron Jobs

Schedule regular ingestion:

```bash
# Every 6 hours
0 */6 * * * cd /path/to/rag && python rag_ingestion.py ingest 1
```

Or use Supabase Edge Functions with cron triggers.

## Performance

- **Embedding creation**: ~200ms per event
- **Location extraction**: ~500ms per event (uses GPT-4o-mini)
- **Semantic search**: <100ms for 10k events
- **Spatial queries**: <50ms with proper indexes

## Location Extraction

The system uses **fast regex matching** by default:
- Searches for "Sector 1-6" patterns
- Matches neighborhood names (CrangaÈ™i, Titan, Militari, etc.)
- No API calls = fast & free

**Optional LLM extraction** (disabled by default):
- Set `use_llm=True` in `extract_location_info()` for complex addresses
- Uses GPT-4o-mini with 5s timeout
- Costs API credits but more accurate

## Performance

- **RSS scraping**: ~5-10s per feed
- **Google News search**: ~1-2s per query (requires Serper API)
- **Event processing**: ~200 events/minute (without LLM location extraction)
- **Embedding creation**: ~200ms per event (OpenAI ada-002)
- **Database insert**: Batch insert, <1s for 100 events

## Limitations

1. **No real-time social media** (requires API access)
2. **Location accuracy**: Regex-based (fast but basic), optional LLM (slower but better)
3. **Language**: Romanian + English keywords
4. **Rate limits**: 
   - Serper API free tier: 2,500 searches/month (60/day)
   - OpenAI embeddings: ~$0.0001 per event

## Future Enhancements

- [ ] Instagram/TikTok via official APIs
- [ ] Telegram channel monitoring
- [ ] Real-time webhook notifications
- [ ] Image analysis for fire/smoke detection
- [ ] Multi-city support
- [ ] Sentiment analysis for severity
- [ ] Deduplication of similar events

## Troubleshooting

**"SERPER_API_KEY not set"** â†’ Add key to `.env` or skip (RSS feeds still work)

**"Tenant or user not found"** â†’ Update Supabase credentials in `.env`

**"Type vector does not exist"** â†’ Run `CREATE EXTENSION IF NOT EXISTS vector;` in Supabase SQL Editor

**Slow ingestion** â†’ Reduce `days_ago` parameter: `python rag_ingestion.py ingest 1`

## Files

- `rag_ingestion.py` - Main scraper script (run with cron)
- `database/events_schema.sql` - Supabase schema (run once)
- `requirements.txt` - Python dependencies
- `README.md` - This file

## License

MIT
